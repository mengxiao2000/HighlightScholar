import streamlit as st
from openai import OpenAI
import PyPDF2
from io import BytesIO
import fitz  # PyMuPDF
from fuzzywuzzy import fuzz  # ç”¨äºæ¨¡ç³ŠåŒ¹é…
import json
import re
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
import rapidfuzz
from rapidfuzz import fuzz
from concurrent.futures import ThreadPoolExecutor

# Streamlitåº”ç”¨
st.title("ğŸ¨ HighlightScholar")

# ç”¨æˆ·è¾“å…¥API Keyå’ŒBase URL
api_key = st.secrets["api_key"] #st.text_input("è¯·è¾“å…¥æ‚¨çš„OpenAI API Key", type="password")
base_url = st.secrets["base_url"] #st.text_input("è¯·è¾“å…¥æ‚¨çš„OpenAI Base URL", )


# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(api_key=api_key, base_url=base_url)
#if api_key and base_url:
#    client = OpenAI(api_key=api_key, base_url=base_url)
#else:
#    st.warning("è¯·è¾“å…¥API Keyå’ŒBase URLä»¥ç»§ç»­ã€‚")
#    st.stop()

# ä»PDFä¸­æå–æ–‡æœ¬
def extract_text_from_pdf(file):
    reader = PyPDF2.PdfReader(file)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text, reader

# å‹ç¼©æ–‡æœ¬
def compress_text(text, target_sentences=60):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, sentences_count=target_sentences)
    compressed_text = ''
    for s in summary:
        compressed_text += ' '.join(s.words) + '. '
    return compressed_text

# ä½¿ç”¨OpenAI GPTæ¨¡å‹æå–å…³é”®å†…å®¹
def extract_key_content(text):
    prompt = f"""
    è¯·ä»ä»¥ä¸‹å­¦æœ¯æ–‡ç« ä¸­æå–ä»¥ä¸‹å…³é”®å†…å®¹ï¼Œå¹¶ä»¥ä¸¥æ ¼çš„JSONæ ¼å¼è¿”å›ï¼š
    {{
        "Background": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶èƒŒæ™¯çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶èƒŒæ™¯çš„å¥å­2ã€‚" ...],
        "Research Gap": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°å·²æœ‰ç ”ç©¶å±€é™çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°å·²æœ‰ç ”ç©¶å±€é™çš„å¥å­2ã€‚" ...],
        "Research Questions/Hypotheses": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶é—®é¢˜æˆ–å‡è®¾çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶é—®é¢˜æˆ–å‡è®¾çš„å¥å­2ã€‚","ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶é—®é¢˜æˆ–å‡è®¾çš„å¥å­3ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶é—®é¢˜æˆ–å‡è®¾çš„å¥å­4ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶é—®é¢˜æˆ–å‡è®¾çš„å¥å­5ã€‚"...],
        "Research Method": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶æ–¹æ³•çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶æ–¹æ³•çš„å¥å­2ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶æ–¹æ³•çš„å¥å­3ã€‚"...],
        "Research Data": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°æ•°æ®æ”¶é›†çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°æ•°æ®æ”¶é›†çš„å¥å­2ã€‚" ...],
        "Main Findings": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶å‘ç°çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶å‘ç°çš„å¥å­2ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶å‘ç°çš„å¥å­3ã€‚",...],
        "Originality/Innovation": ["ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶åˆ›æ–°ç‚¹çš„å¥å­1ã€‚", "ç›´æ¥å¼•ç”¨åŸæ–‡ä¸­æè¿°ç ”ç©¶åˆ›æ–°ç‚¹çš„å¥å­2ã€‚" ...]
    }}

    æ³¨æ„ï¼š
    1. è¯·ç¡®ä¿è¿”å›çš„å†…å®¹æ˜¯åŸæ–‡ä¸­çš„ç›´æ¥å¼•ç”¨ï¼Œä¸è¦æ”¹å˜åŸæ–‡çš„è¯­è¨€å’Œå†…å®¹å½¢å¼ã€‚
    2. ç¡®ä¿è¿”å›çš„å†…å®¹æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼Œé”®å’Œå€¼éƒ½ç”¨åŒå¼•å·æ‹¬èµ·æ¥ã€‚
    3. æ¯ä¸ªå…³é”®å†…å®¹çš„å€¼æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå¥å­çš„åˆ—è¡¨ã€‚
    4. è¯·ä¿è¯å…³é”®å†…å®¹è¢«å…¨é¢æå–ï¼Œæ¯ä¸ªå…³é”®å†…å®¹çš„åˆ—è¡¨ä¸­å€¼çš„æ•°é‡ä¸º1åˆ°10ä¸ªã€‚
    5. é¿å…ä¸åŒå…³é”®å†…å®¹ä¸­å‡ºç°é‡å¤çš„å¥å­ã€‚

    æ–‡ç« å†…å®¹ï¼š
    {text}
    """
    
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯åŠ©æ‰‹ï¼Œèƒ½å¤Ÿä»æ–‡ç« ä¸­æå–å…³é”®å†…å®¹ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2
    )
    
    # æå–è¿”å›çš„å†…å®¹
    result = response.choices[0].message.content.strip()
    return result

def fuzzy_match_sentence(sentences, keyword_list, threshold=55):
    # é¢„å¤„ç†å¥å­å’Œå…³é”®è¯
    sentences_clean = [re.sub(r'[^\w\s]', '', s.lower()) for s in sentences]
    keyword_list_clean = [re.sub(r'[^\w\s]', '', k.lower()) for k in keyword_list]
    
    # ç”¨äºå­˜å‚¨åŒ¹é…çš„å¥å­
    matched_sentences = set()  # ä½¿ç”¨é›†åˆé¿å…é‡å¤
    
    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåŒ¹é…å•ä¸ªå…³é”®è¯
    def match_keyword(keyword_clean):
        matched = []
        for i, sentence_clean in enumerate(sentences_clean):
            # æå‰è¿‡æ»¤ï¼šå¦‚æœå¥å­é•¿åº¦ä¸å…³é”®è¯é•¿åº¦å·®å¼‚è¿‡å¤§ï¼Œè·³è¿‡
            if abs(len(sentence_clean) - len(keyword_clean)) > 20:
                continue
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = fuzz.token_sort_ratio(sentence_clean, keyword_clean)
            if similarity >= threshold:
                matched.append(sentences[i])  # ä½¿ç”¨åŸå§‹å¥å­
        return matched
    
    # ä½¿ç”¨å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†å…³é”®è¯
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(match_keyword, keyword_clean) for keyword_clean in keyword_list_clean]
        for future in futures:
            matched_sentences.update(future.result())
    
    return list(matched_sentences)

# åœ¨PDFä¸­é«˜äº®å…³é”®å†…å®¹
def highlight_pdf(pdf_bytes, key_content, text):
    # æ‰“å¼€PDFæ–‡ä»¶ï¼ˆç›´æ¥ä»å†…å­˜ä¸­è¯»å–ï¼‰
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    
    # è§£ææå–çš„å…³é”®å†…å®¹
    try:
        key_content = key_content.strip('```json').strip()
        key_content_dict = json.loads(key_content)
    except json.JSONDecodeError as e:
        st.error(f"æå–çš„å…³é”®å†…å®¹æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•é«˜äº®ã€‚é”™è¯¯ä¿¡æ¯ï¼š{e}")
        return None
    
    
    # å®šä¹‰ä¸åŒå…³é”®å†…å®¹çš„é«˜äº®é¢œè‰²
    highlight_colors = {
        "Background": (0.95, 0.6, 0.6),  # æŸ”å’Œçš„çº¢è‰²
        "Research Gap": (1.0, 0.7, 0.4),  # æŸ”å’Œçš„æ©™è‰²
        "Research Questions/Hypotheses": (0.6, 0.95, 0.6),  # æŸ”å’Œçš„ç»¿è‰²
        "Research Method": (0.95, 0.95, 0.6),  # æŸ”å’Œçš„é»„è‰²
        "Research Data": (0.6, 0.6, 0.95),  # æŸ”å’Œçš„è“è‰²
        "Main Findings": (0.6, 0.95, 0.95),  # æŸ”å’Œçš„é’è‰²
        "Originality/Innovation": (0.95, 0.6, 0.95),  # æŸ”å’Œçš„ç´«è‰²
    }
    
    # åœ¨ç¬¬ä¸€é¡µæ·»åŠ è‰²å—å’Œæ³¨é‡Š
    first_page = doc.load_page(0)
    page_width = first_page.rect.width
    page_height = first_page.rect.height
    
    # è‰²å—å’Œæ³¨é‡Šçš„èµ·å§‹ä½ç½®
    x0 = page_width - 150  # è‰²å—èµ·å§‹Xåæ ‡
    y0 = 10  # è‰²å—èµ·å§‹Yåæ ‡
    block_height = 5  # æ¯ä¸ªè‰²å—çš„é«˜åº¦
    spacing = 5  # è‰²å—ä¹‹é—´çš„é—´è·
    
    for key, color in highlight_colors.items():
        # ç»˜åˆ¶è‰²å—
        rect = fitz.Rect(x0, y0, x0 + 30, y0 + block_height)
        first_page.draw_rect(rect, color=color, fill=color)

        # æ·»åŠ æ–‡æœ¬
        text_point = (x0 + 40, y0 + block_height / 2 + 5)  # æ–‡æœ¬ä½ç½®ï¼ˆè‰²å—å³ä¾§å±…ä¸­ï¼‰
        first_page.insert_text(
            text_point,  # æ–‡æœ¬æ’å…¥ä½ç½®
            key,  # æ–‡æœ¬å†…å®¹
            fontsize=6,  # å­—ä½“å¤§å°
            fontname="helv",  # å­—ä½“åç§°
            color=(0, 0, 0)  # æ–‡æœ¬é¢œè‰²ï¼ˆé»‘è‰²ï¼‰
        )

        # æ›´æ–°ä¸‹ä¸€ä¸ªè‰²å—çš„Yåæ ‡
        y0 += block_height + spacing
    
    # éå†æ¯ä¸€é¡µ
    for page_num in range(len(doc)):
        
        # åˆå§‹åŒ–ä¸€ä¸ªlistï¼Œç”¨äºè®°å½•å·²ç»é«˜äº®çš„å¥å­
        highlighted_sentences = []
    
        page = doc.load_page(page_num)
        page_text = page.get_text().replace('\n', ' ').replace('  ', ' ')  # è·å–å½“å‰é¡µçš„æ–‡æœ¬
        
        # å°†æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?]', page_text)
        sentences = [s.strip() for s in sentences if s.strip()]
    
        if not page_text.strip():  # æ£€æŸ¥é¡µé¢æ–‡æœ¬æ˜¯å¦ä¸ºç©º
            st.warning(f"ç¬¬ {page_num + 1} é¡µçš„æ–‡æœ¬ä¸å¯æœç´¢ï¼Œå¯èƒ½æ˜¯å›¾åƒæˆ–æ‰«æä»¶ã€‚")
            continue
        
        # éå†å…³é”®å†…å®¹å¹¶é«˜äº®
        for key, value in key_content_dict.items():
            if isinstance(value, list):  # ç¡®ä¿å€¼æ˜¯åˆ—è¡¨
                # ä½¿ç”¨å¥å­çº§åˆ«çš„æ¨¡ç³ŠåŒ¹é…æŸ¥æ‰¾ç›¸ä¼¼å¥å­
                matched_sentences = list(set(fuzzy_match_sentence(sentences, value)))
                
                for sentence in matched_sentences:
                    if len(sentence.split(' ')) < 5:
                        continue 
                    
                    if sentence not in highlighted_sentences:
                        highlighted_sentences.append(sentence)
                    
                        # æŸ¥æ‰¾åŒ¹é…å¥å­åœ¨é¡µé¢ä¸­çš„ä½ç½®
                        text_instances = page.search_for(sentence)

                        for inst in text_instances:
                            # æ·»åŠ é«˜äº®æ³¨é‡Šï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„é¢œè‰²
                            highlight = page.add_highlight_annot(inst)
                            highlight.set_colors(stroke=highlight_colors.get(key, (1, 1, 0)))  # é»˜è®¤é»„è‰²
                            highlight.update()
    
    # å°†é«˜äº®åçš„PDFä¿å­˜åˆ°å†…å­˜ä¸­
    output_bytes = BytesIO()
    doc.save(output_bytes)
    doc.close()
    output_bytes.seek(0)
    
    return output_bytes

# ä¸Šä¼ PDFæ–‡ä»¶
uploaded_file = st.file_uploader("ä¸Šä¼ å­¦æœ¯æ–‡ç«  (PDFæ ¼å¼)", type="pdf")
if uploaded_file is not None:
    # å°†ä¸Šä¼ çš„æ–‡ä»¶è¯»å–åˆ°å†…å­˜ä¸­
    pdf_bytes = uploaded_file.read()
    
    # æå–æ–‡æœ¬
    text, pdf_reader = extract_text_from_pdf(BytesIO(pdf_bytes))
    
    # æ˜¾ç¤ºæå–çš„æ–‡æœ¬
    st.write("### æå–çš„æ–‡æœ¬")
    st.write(text[:5000] + "...")  # æ˜¾ç¤ºå‰5000å­—ç¬¦ï¼ˆé¿å…é¡µé¢è¿‡é•¿ï¼‰

    # æ·»åŠ  "Highlight" æŒ‰é’®
    if st.button("Highlight"):
        with st.spinner("æ­£åœ¨æå–å…³é”®å†…å®¹å¹¶é«˜äº®..."):
            # æå–å…³é”®å†…å®¹
            key_content = extract_key_content(text)
            st.write("### æå–çš„å…³é”®å†…å®¹")
            st.write(key_content)

            # é«˜äº®PDFä¸­çš„å…³é”®å†…å®¹
            highlighted_pdf = highlight_pdf(pdf_bytes, key_content, text)
            
            if highlighted_pdf:
                # æä¾›ä¸‹è½½é“¾æ¥
                st.download_button(
                    label="ä¸‹è½½é«˜äº®PDF",
                    data=highlighted_pdf.getvalue(),  # è·å–å­—èŠ‚æµçš„å€¼
                    file_name="highlighted_article.pdf",
                    mime="application/pdf"
                )
                
                # é¢„è§ˆé«˜äº®PDF
                st.write("### é«˜äº®PDFé¢„è§ˆ")
                st.markdown("ç”±äºStreamlitä¸æ”¯æŒç›´æ¥é¢„è§ˆPDFæ–‡ä»¶ï¼Œè¯·ä¸‹è½½åæŸ¥çœ‹é«˜äº®æ•ˆæœã€‚")
